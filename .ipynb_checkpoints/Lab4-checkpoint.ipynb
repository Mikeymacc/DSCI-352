{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9089c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# defines the mapreduce function\n",
    "def mapreduce(mapper, reducer, data):\n",
    "    # map phase\n",
    "    mdata = [pair for item in data for pair in mapper(item)]\n",
    "\n",
    "    # shuffles and sorts\n",
    "    sdata = defaultdict(list)\n",
    "    for key, value in mdata:\n",
    "        sdata[key].append(value)\n",
    "    sitems = sdata.items()\n",
    "\n",
    "    # reduces the data\n",
    "    rdata = [reducer(item) for item in sitems]\n",
    "    rdata = [x for x in rdata if x is not None]  # Drop None pairs\n",
    "\n",
    "    return rdata, [len(item) for item in data], len(mdata), len(sdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec7bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines the mapper function for word counting\n",
    "def wcmapper(data):\n",
    "    key_value_pairs = []\n",
    "    for line in data:\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            key_value_pairs.append((word.lower(), 1))  # Convert to lowercase for uniformity\n",
    "    return key_value_pairs\n",
    "\n",
    "# Define the reducer function for word counting\n",
    "def wcreducer(key_value):\n",
    "    key, values = key_value\n",
    "    return (key, sum(values))  # Sum all counts for each word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20159e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single Chunk Result: [('hello', 2), ('world', 2), ('mapreduce', 2), ('is', 1), ('powerful', 1), ('and', 1)]\n",
      "Multiple Chunks Result: [('hello', 2), ('world', 2), ('mapreduce', 2), ('is', 1), ('powerful', 1), ('and', 1)]\n"
     ]
    }
   ],
   "source": [
    "# sample input data\n",
    "input_texts = [\n",
    "    \"Hello world\",\n",
    "    \"MapReduce is powerful\",\n",
    "    \"Hello world and MapReduce\"\n",
    "]\n",
    "\n",
    "# mapreduce with input as a single chunk\n",
    "result_singlechunk, _, _, _ = mapreduce(\n",
    "    lambda data: wcmapper(data),\n",
    "    lambda item: wcreducer(item),\n",
    "    [input_texts]\n",
    ")\n",
    "\n",
    "# mapreduce with input split into multiple chunks\n",
    "result_multchunks, _, _, _ = mapreduce(\n",
    "    lambda data: wcmapper(data),\n",
    "    lambda item: wcreducer(item),\n",
    "    [[line] for line in input_texts]  # splits single-line chunks\n",
    ")\n",
    "\n",
    "# displays results\n",
    "print(\"Single Chunk Result:\", result_singlechunk)\n",
    "print(\"Multiple Chunks Result:\", result_multchunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e78209be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Common Items:\n",
      "soda           1514\n",
      "yogurt         1334\n",
      "sausage         924\n",
      "pastry          785\n",
      "newspapers      596\n",
      "frankfurter     580\n",
      "pork            566\n",
      "butter          534\n",
      "beef            516\n",
      "curd            514\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Least Common Items:\n",
      "syrup          21\n",
      "soap           20\n",
      "prosecco       19\n",
      "cookware       17\n",
      "honey          13\n",
      "cream          12\n",
      "liqueur         9\n",
      "decalcifier     9\n",
      "whisky          8\n",
      "bags            4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# loads in data\n",
    "basket_data = pd.read_csv(\"basket.csv\", header=None)\n",
    "\n",
    "# flattens the dataset into array and count occurrences of each item\n",
    "all_items = basket_data.values.flatten()  \n",
    "all_items = pd.Series(all_items).dropna()  # Drop nan values\n",
    "all_items = all_items[all_items.str.isalpha()]\n",
    "\n",
    "# frequency of the items\n",
    "item_counts = all_items.value_counts()\n",
    "\n",
    "# 10 most and least common items\n",
    "mostcitems = item_counts.head(10)\n",
    "leastcitems = item_counts.tail(10)\n",
    "\n",
    "# results\n",
    "print(\"Most Common Items:\")\n",
    "print(mostcitems)\n",
    "\n",
    "print(\"\\nLeast Common Items:\")\n",
    "print(leastcitems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e989572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# converts each row to a list of items\n",
    "basketlist = basket_data.apply(lambda row: row.dropna().tolist(), axis=1).tolist()\n",
    "\n",
    "# one-hot encode the data\n",
    "te = TransactionEncoder()\n",
    "basketoneh = te.fit_transform(basketlist)\n",
    "basketoneh_df = pd.DataFrame(basketoneh, columns=te.columns_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "984a0c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Support Threshold: 0.01\n",
      "Number of Frequent Itemsets: 69\n",
      "    support       itemsets\n",
      "0  0.157912   (whole milk)\n",
      "1  0.051724       (pastry)\n",
      "2  0.018778  (salty snack)\n",
      "3  0.085873       (yogurt)\n",
      "4  0.060345      (sausage)\n",
      "\n",
      "Support Threshold: 0.005\n",
      "Number of Frequent Itemsets: 126\n",
      "    support       itemsets\n",
      "0  0.157912   (whole milk)\n",
      "1  0.051724       (pastry)\n",
      "2  0.018778  (salty snack)\n",
      "3  0.085873       (yogurt)\n",
      "4  0.060345      (sausage)\n",
      "\n",
      "Support Threshold: 0.001\n",
      "Number of Frequent Itemsets: 750\n",
      "    support       itemsets\n",
      "0  0.157912   (whole milk)\n",
      "1  0.051724       (pastry)\n",
      "2  0.018778  (salty snack)\n",
      "3  0.085873       (yogurt)\n",
      "4  0.060345      (sausage)\n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "# different thresholds\n",
    "thresholds = [0.01, 0.005, 0.001]  # range of thresholds\n",
    "\n",
    "for support in thresholds:\n",
    "    frequent_itemsets = fpgrowth(basketoneh_df, min_support=support, use_colnames=True)\n",
    "    print(f\"\\nSupport Threshold: {support}\")\n",
    "    print(f\"Number of Frequent Itemsets: {len(frequent_itemsets)}\")\n",
    "    print(frequent_itemsets.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89fbc39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association Rules with at least 2 items in antecedents or consequents:\n",
      "                 antecedents         consequents  antecedent support  \\\n",
      "0          (sausage, yogurt)        (whole milk)            0.005747   \n",
      "1  (rolls/buns, white bread)        (whole milk)            0.002138   \n",
      "2   (sausage, shopping bags)  (other vegetables)            0.001938   \n",
      "4            (sausage, pork)        (whole milk)            0.001537   \n",
      "\n",
      "   consequent support   support  confidence      lift  representativity  \\\n",
      "0            0.157912  0.001470    0.255814  1.619975               1.0   \n",
      "1            0.157912  0.000601    0.281250  1.781052               1.0   \n",
      "2            0.122093  0.000535    0.275862  2.259442               1.0   \n",
      "4            0.157912  0.000601    0.391304  2.477985               1.0   \n",
      "\n",
      "   leverage  conviction  zhangs_metric   jaccard  certainty  kulczynski  \n",
      "0  0.000563    1.131555       0.384919  0.009065   0.116261    0.132562  \n",
      "1  0.000264    1.171600       0.439474  0.003772   0.146467    0.142529  \n",
      "2  0.000298    1.212348       0.558495  0.004329   0.175154    0.140120  \n",
      "4  0.000359    1.383430       0.597364  0.003786   0.277159    0.197557  \n"
     ]
    }
   ],
   "source": [
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# uses fpgrowth to compute frequent itemsets with a min support of 0.0005\n",
    "frequentitemsets = fpgrowth(basketoneh_df, min_support=0.0005, use_colnames=True)\n",
    "\n",
    "numis = len(frequentitemsets)\n",
    "\n",
    "# generates association rules with min conf of 0.5\n",
    "rules = association_rules(frequentitemsets, metric=\"confidence\", min_threshold=0.25, num_itemsets=numis)\n",
    "\n",
    "# filters rules where the antecedent or consequent has at least 2 items\n",
    "filtrules = rules[(rules['antecedents'].apply(lambda x: len(x) >= 2)) | (rules['consequents'].apply(lambda x: len(x) >= 2))]\n",
    "\n",
    "# display the resulting association rules\n",
    "print(\"Association Rules with at least 2 items in antecedents or consequents:\")\n",
    "print(filtrules)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ffeaf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "# mapper function for frequent itemsets\n",
    "def frequentismapper(data_chunk):\n",
    "    # computes frequent itemsets on the chunk\n",
    "    frequentis = fpgrowth(data_chunk, min_support=0.0005, use_colnames=True)\n",
    "    \n",
    "    # generates key,value pairs\n",
    "    kvpairs = []\n",
    "    for _, row in frequentis.iterrows():\n",
    "        itemset = frozenset(row['itemsets'])  # key: itemset as a frozenset\n",
    "        cnt = row['support'] * len(data_chunk)  # value: count derived from support\n",
    "        kvpairs.append((itemset, cnt))\n",
    "    return kvpairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7b0f961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reducer function\n",
    "def frequentisreducer(key_value):\n",
    "    key, values = key_value  # key: itemset, values: list of counts\n",
    "    ts = sum(values)\n",
    "    if ts >= 0.0005:  # checks threshold\n",
    "        return (key, ts)  #returns itemset and total support\n",
    "    return None  # or excludes ones below the threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea00c9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# defines splits \n",
    "splits = [2, 4, 8, 16]\n",
    "moutputs = []\n",
    "skeys = []\n",
    "\n",
    "for nchunks in splits:\n",
    "    # Split the one-hot encoded DataFrame into chunks\n",
    "    chunkeddata = np.array_split(basketoneh_df, nchunks)\n",
    "    \n",
    "    # Run MapReduce on the chunks\n",
    "    _, mapper_lengths, lenm, lens = mapreduce(\n",
    "        lambda data: frequentismapper(data),\n",
    "        lambda item: frequentisreducer(item),\n",
    "        chunkeddata\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    moutputs.append(lenm)\n",
    "    skeys.append(lens)\n",
    "\n",
    "# verifies consistency with direct computation\n",
    "frequent_itemsets_full = fpgrowth(basketoneh_df, min_support=0.0005, use_colnames=True)\n",
    "print(f\"Number of Frequent Itemsets from Full Data: {len(frequent_itemsets_full)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6ae60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(splits, mapper_outputs, label='Mapper Outputs', marker='o')\n",
    "plt.plot(splits, shuffled_keys, label='Distinct Keys', marker='x')\n",
    "plt.xlabel('Number of Splits')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mapper Outputs vs. Distinct Keys')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ebde3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
